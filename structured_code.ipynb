{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cda160c-0af0-4739-82fe-8f62dd660ace",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6632fc29-92ce-4411-8e77-48f32304624a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import APIRouter, UploadFile, File\n",
    "import asyncio\n",
    "import io\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from app.backend.utlis.ocr import pan_ocr, adhar_ocr\n",
    "from app.backend.utlis.face import extract_face, enhance_image, match_faces_with_facenet\n",
    "from app.backend.utlis.logging_config import get_logger  # Import the global logger\n",
    "from typing import Optional, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f0ed176-7fe3-4348-bad7-75bbaed81386",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82d71e40-c0a2-4905-bcbb-1cfa2c513952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(image_data: bytes) -> Optional[np.ndarray]:\n",
    "    logger.info(f\"Received image data, length: {len(image_data)} bytes\")\n",
    "    if not image_data:\n",
    "        logger.error(\"Image data is empty or unreadable\")\n",
    "        return None\n",
    "    try:\n",
    "        image = Image.open(io.BytesIO(image_data))\n",
    "        cv2_img = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "        logger.info(f\"Image read successfully with shape {cv2_img.shape}\")\n",
    "        return cv2_img\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error decoding image data: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37f83fb0-b15f-49df-b4ec-33fd2342daee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents() -> Dict[str, dict]:\n",
    "    logger.info(\"Processing documents started\")\n",
    "    try:\n",
    "        with open(\"pan.jpg\", \"rb\") as image_file:\n",
    "            pan_image_data = image_file.read()\n",
    "        with open(\"live.jpg\", \"rb\") as image_file:\n",
    "            live_image_data = image_file.read()\n",
    "        with open(\"adhar_front.jpg\", \"rb\") as image_file:\n",
    "            adhar_image_data = image_file.read()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading images: {e}\")\n",
    "        return {\"error\": \"Failed to read images\"}\n",
    "    try:\n",
    "        pan_img = read_image(pan_image_data)\n",
    "        adhar_img = read_image(adhar_image_data)\n",
    "        live_img = read_image(live_image_data)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading image data: {e}\")\n",
    "        return {\"error\": \"Error processing image data\"}\n",
    "    extracted_face = extract_face(live_img)\n",
    "    if extracted_face is None:\n",
    "        logger.warning(\"No face detected in live image\")\n",
    "        return {\"error\": \"No face detected in live image\"}\n",
    "    logger.info(\"Face successfully extracted from live image\")\n",
    "    try:\n",
    "        pan_text= pan_ocr(image=pan_img)\n",
    "        adhar_text=adhar_ocr(image=adhar_img)\n",
    "        logger.info(\"OCR completed successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"OCR processing error: {e}\")\n",
    "        return {\"error\": \"Failed to perform OCR\"}\n",
    "    adhar_face=extract_face(adhar_img)\n",
    "    pan_face =extract_face(pan_img)\n",
    "    if adhar_face is not None:\n",
    "        logger.info(\"Face extracted from Aadhar image\")\n",
    "    else:\n",
    "        logger.warning(\"No face detected in Aadhar image\")\n",
    "    if pan_face is not None:\n",
    "        logger.info(\"Face extracted from PAN image\")\n",
    "    else:\n",
    "        logger.warning(\"No face detected in PAN image\")\n",
    "    match_adhar, score_adhar = match_faces_with_facenet(extracted_face, adhar_face) if adhar_face is not None else (False, None)\n",
    "    match_pan, score_pan = match_faces_with_facenet(extracted_face, pan_face) if pan_face is not None else (False, None)\n",
    "    logger.info(f\"Face match results - Aadhar: {match_adhar}, Score: {score_adhar}\")\n",
    "    logger.info(f\"Face match results - PAN: {match_pan}, Score: {score_pan}\")\n",
    "    return {\n",
    "        \"pan_ocr\": pan_text,\n",
    "        \"adhar_ocr\": adhar_text,\n",
    "        \"face_match_with_adhar\": {\"match\": match_adhar, \"score\": score_adhar},\n",
    "        \"face_match_with_pan\": {\"match\": match_pan, \"score\": score_pan},\n",
    "        \"extracted_pan_face\" : pan_face,\n",
    "        \"extracted_adhar_face\" : adhar_face,\n",
    "        \"pan_img\" : pan_img,\n",
    "        \"adhar_img\" : adhar_img,\n",
    "        \"live_img\" : live_img,\n",
    "        \"extracted_live_face\" : extracted_face,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff4ca6fb-fe58-4f3d-8d4f-a29be2761096",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pan.jpg\", \"rb\") as image_file:\n",
    "    pan_image_data = image_file.read()\n",
    "\n",
    "with open(\"live.jpg\", \"rb\") as image_file:\n",
    "    live_image_data = image_file.read()\n",
    "\n",
    "with open(\"adhar_front.jpg\", \"rb\") as image_file:\n",
    "    adhar_image_data = image_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e15f3b-af35-4dd4-9aed-9778d67f0d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=process_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a047ce16-5c7c-48c6-a77f-9786ae4cc530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "arr_rgb = cv2.cvtColor(result['extracted_live_face'], cv2.COLOR_BGR2RGB)\n",
    "image = Image.fromarray(arr_rgb)\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5443ea88-42c1-40bd-892d-11976a3e155c",
   "metadata": {},
   "source": [
    "# extract face from id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "180a58f3-b30b-44cb-b69c-1d0d0eacd8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from mtcnn import MTCNN\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def extract_and_save_face(image_path: str, *, save_path: str = \"cropped_faces/\", padding: float = 0.35) -> bool:\n",
    "    \"\"\"\n",
    "    Detects a face in the given image, applies padding to the cropped face, saves it, and returns whether a face was found.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the input image.\n",
    "        save_path (str, keyword-only): Directory where cropped face images will be saved.\n",
    "        padding (float, keyword-only): Padding factor (0.1 = 10% extra around the face).\n",
    "\n",
    "    Returns:\n",
    "        bool: True if a face was found and saved, False otherwise.\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    image: Optional[np.ndarray] = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to read image {image_path}\")\n",
    "        return False\n",
    "\n",
    "    # Initialize MTCNN detector\n",
    "    detector = MTCNN()\n",
    "\n",
    "    # Detect faces\n",
    "    faces = detector.detect_faces(image)\n",
    "\n",
    "    if not faces:\n",
    "        print(\"No face detected.\")\n",
    "        return False\n",
    "\n",
    "    # Ensure save directory exists\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    height, width, _ = image.shape  # Image dimensions\n",
    "\n",
    "    for i, face in enumerate(faces):  # Loop in case there are multiple faces\n",
    "        x, y, w, h = face[\"box\"]\n",
    "\n",
    "        # Compute padding\n",
    "        pad_x = int(w * padding)\n",
    "        pad_y = int(h * padding)\n",
    "\n",
    "        # Expand bounding box\n",
    "        x1, y1 = max(x - pad_x, 0), max(y - pad_y, 0)\n",
    "        x2, y2 = min(x + w + pad_x, width), min(y + h + pad_y, height)\n",
    "\n",
    "        # Crop the padded face\n",
    "        cropped_face = image[y1:y2, x1:x2]\n",
    "\n",
    "        # Save cropped face\n",
    "        face_filename = os.path.join(save_path, f\"face_{os.path.basename(image_path)}\")\n",
    "        cv2.imwrite(face_filename, cropped_face)\n",
    "        print(f\"✅ Face saved at: {face_filename} with padding {padding*100:.1f}%\")\n",
    "        return True  # Return True after saving the first detected face\n",
    "\n",
    "    return False  # If no face was saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4d270e53-1d2e-4698-a79a-ab0aaad70476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Face saved at: cropped_faces/face_live.jpg with padding 35.0%\n",
      "Face extraction successful: True\n"
     ]
    }
   ],
   "source": [
    "success = extract_and_save_face(\"live.jpg\")\n",
    "print(f\"Face extraction successful: {success}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b596a75a-1e02-4852-9c87-d6251935629b",
   "metadata": {},
   "source": [
    "# esrGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6e479ebd-2d73-45b5-9e2b-08835e8cbe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "\n",
    "\n",
    "# Load Pretrained ESRGAN Model from TensorFlow Hub\n",
    "srgan_model = hub.load(\"https://tfhub.dev/captain-pool/esrgan-tf2/1\")\n",
    "\n",
    "\n",
    "def enhance_image(image_path: str, *, save_path: str = \"enhanced_faces/\") -> bool:\n",
    "    \"\"\"\n",
    "    Enhances the quality of a face image using a Super-Resolution GAN (SRGAN).\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the cropped face image.\n",
    "        save_path (str, keyword-only): Directory where enhanced images will be saved.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if enhancement is successful, False otherwise.\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to read image {image_path}\")\n",
    "        return False\n",
    "\n",
    "    # Convert to RGB and normalize (0-1)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0\n",
    "    #image/=255.0\n",
    "    # Resize to match ESRGAN input requirements\n",
    "    #image_resized = cv2.resize(image, (50, 50))  # Model expects 50x50 input\n",
    "    image_resized = np.expand_dims(image, axis=0).astype(np.float32)\n",
    "\n",
    "    # Run through the ESRGAN model\n",
    "    enhanced = srgan_model(image_resized)[0]  # Get first output\n",
    "\n",
    "    # ✅ Convert EagerTensor to NumPy before processing\n",
    "    enhanced = np.array(enhanced)  \n",
    "    enhanced = (enhanced * 255.0).clip(0, 255).astype(np.uint8)  # Process as usual\n",
    "\n",
    "    # Ensure save directory exists\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    # Save enhanced image\n",
    "    enhanced_filename = os.path.join(save_path, f\"enhanced_{os.path.basename(image_path)}\")\n",
    "    cv2.imwrite(enhanced_filename, cv2.cvtColor(enhanced, cv2.COLOR_RGB2BGR))\n",
    "    print(f\"✅ Enhanced face saved at: {enhanced_filename}\")\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bca29124-689e-485e-ac70-41022d7035f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Enhanced face saved at: enhanced_faces/enhanced_face_live.jpg\n",
      "Super-resolution enhancement successful: True\n"
     ]
    }
   ],
   "source": [
    "success = enhance_image(\"cropped_faces/face_live.jpg\")\n",
    "print(f\"Super-resolution enhancement successful: {success}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284a8476-60b6-442c-93a3-b8be6eb151fd",
   "metadata": {},
   "source": [
    "# Face Matching- FaceNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "41bd687b-c57c-4eb1-89eb-f309b8785e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_facenet import FaceNet\n",
    "import numpy as np\n",
    "import cv2\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def match_faces_with_facenet(image1_path: str, image2_path: str):\n",
    "    \"\"\"\n",
    "    Compares two pre-processed faces and returns whether they match and the similarity score.\n",
    "\n",
    "    Args:\n",
    "        image1_path (str): Path to the first image (already cropped and enhanced).\n",
    "        image2_path (str): Path to the second image (already cropped and enhanced).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - bool: True if the faces match, False otherwise.\n",
    "            - float: The cosine similarity score between the two faces.\n",
    "    \"\"\"\n",
    "    # Load the images using OpenCV (ensure they are valid)\n",
    "    image1 = cv2.imread(image1_path)\n",
    "    image2 = cv2.imread(image2_path)\n",
    "    \n",
    "    if image1 is None:\n",
    "        raise ValueError(f\"Failed to load image at {image1_path}\")\n",
    "    if image2 is None:\n",
    "        raise ValueError(f\"Failed to load image at {image2_path}\")\n",
    "\n",
    "    # Resize the images to the size that FaceNet expects (160x160)\n",
    "    image1 = cv2.resize(image1, (160, 160))\n",
    "    image2 = cv2.resize(image2, (160, 160))\n",
    "\n",
    "    # Convert images to RGB (FaceNet expects RGB images, OpenCV loads in BGR by default)\n",
    "    image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "    image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Add an extra batch dimension: from (160, 160, 3) to (1, 160, 160, 3)\n",
    "    image1 = np.expand_dims(image1, axis=0)\n",
    "    image2 = np.expand_dims(image2, axis=0)\n",
    "\n",
    "    # Initialize the FaceNet model\n",
    "    facenet = FaceNet()\n",
    "\n",
    "    # Extract face embeddings\n",
    "    embedding1 = facenet.embeddings(image1)[0]\n",
    "    embedding2 = facenet.embeddings(image2)[0]\n",
    "\n",
    "    # Compute the cosine distance between the two embeddings\n",
    "    distance = cosine(embedding1, embedding2)\n",
    "\n",
    "    # Define a threshold for matching (typically, 0.6 to 0.7 is a common threshold)\n",
    "    threshold = 0.6\n",
    "\n",
    "    # Determine if faces match\n",
    "    faces_match = distance < threshold\n",
    "\n",
    "    return faces_match, distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f9487666-6c5f-4c06-a598-dd47283e5c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step\n",
      "Do the faces match? True\n",
      "Similarity Score (distance): 0.4152754545211792\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "image1_path = \"enhanced_faces/enhanced_face_pan.jpg\"\n",
    "image2_path = \"enhanced_faces/enhanced_face_live.jpg\"\n",
    "\n",
    "faces_match, similarity_score = match_faces_with_facenet(image1_path, image2_path)\n",
    "\n",
    "print(f\"Do the faces match? {faces_match}\")\n",
    "print(f\"Similarity Score (distance): {similarity_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2c2b3d-78b8-4220-ab60-210b6d3f2da3",
   "metadata": {},
   "source": [
    "# OCR using easyocr "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f932037-4c11-4ff0-91c8-2dc170a4aae5",
   "metadata": {},
   "source": [
    "### PAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "43f7634a-83b2-4e61-9324-37e20738117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import easyocr\n",
    "import re\n",
    "import logging\n",
    "\n",
    "# Configure logging (PEP 282)\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "\n",
    "def preprocess_image(image_path: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Preprocess the image by converting it to grayscale and applying adaptive thresholding.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Preprocessed thresholded image.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "\n",
    "    if img is None:\n",
    "        logging.error(f\"Failed to load image: {image_path}\")\n",
    "        raise FileNotFoundError(f\"Image not found at {image_path}\")\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply adaptive thresholding to enhance text visibility\n",
    "    thresh = cv2.adaptiveThreshold(\n",
    "        gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 31, 2\n",
    "    )\n",
    "\n",
    "    logging.info(\"Image preprocessing completed.\")\n",
    "    return thresh\n",
    "\n",
    "\n",
    "def extract_pan_details(*, image_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extract important details from a PAN card image using OCR.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        dict: Extracted information including ID Number, Date of Birth, Name, and Father's Name.\n",
    "    \"\"\"\n",
    "    reader = easyocr.Reader([\"en\"])  # Load OCR reader for English\n",
    "\n",
    "    # Preprocess the image\n",
    "    processed_img = preprocess_image(image_path)\n",
    "\n",
    "    # Extract text using EasyOCR\n",
    "    results = reader.readtext(processed_img, detail=0)  # Extract text only\n",
    "\n",
    "    extracted_info = {\n",
    "        \"ID Number\": None,\n",
    "        \"Date of Birth\": None,\n",
    "        \"Name\": None,\n",
    "        \"Father's Name\": None,\n",
    "    }\n",
    "\n",
    "    name_found = False  # Track if Name is found\n",
    "    father_name_found = False  # Track if Father's Name is found\n",
    "\n",
    "    for i, text in enumerate(results):\n",
    "        text = text.strip()\n",
    "\n",
    "        # Extract PAN Number (Format: ABCDE1234F)\n",
    "        if re.match(r\"^[A-Z]{5}[0-9]{4}[A-Z]$\", text):\n",
    "            extracted_info[\"ID Number\"] = text\n",
    "\n",
    "        # Extract Date of Birth (Format: DD/MM/YYYY)\n",
    "        if re.match(r\"\\d{2}/\\d{2}/\\d{4}\", text):\n",
    "            extracted_info[\"Date of Birth\"] = text\n",
    "\n",
    "        # Extract Name (Assuming it appears after \"Name\" keyword)\n",
    "        if \"name\" in text.lower() and i + 1 < len(results) and not name_found:\n",
    "            extracted_info[\"Name\"] = results[i + 1].strip()\n",
    "            name_found = True\n",
    "\n",
    "        # Extract Father's Name (Appears after \"Father's Name\" or \"पिता का नाम\")\n",
    "        if \"father\" in text.lower() and i + 1 < len(results) and not father_name_found:\n",
    "            extracted_info[\"Father's Name\"] = results[i + 1].strip()\n",
    "            father_name_found = True\n",
    "\n",
    "    logging.info(f\"Extracted details: {extracted_info}\")\n",
    "    return extracted_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ef8ccd-3ec4-44c2-b145-e4fa5326b3e7",
   "metadata": {},
   "source": [
    "### adhar card front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bf2c2e4-d34d-4237-9232-57314f12fa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ext(image_path):\n",
    "    \"\"\"\n",
    "    Extract important details from an Aadhaar card image using OCR.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        dict: Extracted information including Aadhaar Number, Name, Date of Birth, and Gender.\n",
    "    \"\"\"\n",
    "    reader = easyocr.Reader([\"en\"])  # Load OCR reader for English & Hindi\n",
    "\n",
    "    # Preprocess the image\n",
    "    processed_img = preprocess_image(image_path)\n",
    "\n",
    "    # Extract text using EasyOCR\n",
    "    results = reader.readtext(processed_img, detail=0)  # Extract text only\n",
    "\n",
    "    extracted_info = {\n",
    "        \"Aadhaar Number\": None,\n",
    "        \"Date of Birth\": None,\n",
    "        \"Gender\": None,\n",
    "    }\n",
    "\n",
    "    name_candidates = []  # Track if Name is found\n",
    "    gender_found = False  # Track if Gender is found\n",
    "    \n",
    "    for i, text in enumerate(results):\n",
    "        text = text.strip()\n",
    "\n",
    "        # Extract Aadhaar Number (Format: 12-digit number)\n",
    "        if re.match(r\"^\\d{4}\\s?\\d{4}\\s?\\d{4}$\", text):\n",
    "            extracted_info[\"Aadhaar Number\"] = text.replace(\" \", \"\")  # Remove spaces\n",
    "\n",
    "        # Extract Date of Birth (Format: DD/MM/YYYY)\n",
    "        match=re.search(r\"\\b(\\d{2}/\\d{2}/\\d{4})\\b\", text)\n",
    "        if match:\n",
    "            extracted_info[\"Date of Birth\"] = match.group(1)\n",
    "\n",
    "\n",
    "\n",
    "        # Extract Gender (Look for \"Male\", \"Female\" or \"पुरुष\", \"महिला\")\n",
    "        if not gender_found:\n",
    "            if \"male\" in text.lower():\n",
    "                extracted_info[\"Gender\"] = \"Male\"\n",
    "                gender_found = True\n",
    "            elif \"female\" in text.lower():\n",
    "                extracted_info[\"Gender\"] = \"Female\"\n",
    "                gender_found = True\n",
    "\n",
    "    return extracted_info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2207be3f-7ffc-430b-874e-8704b133b539",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    }
   ],
   "source": [
    "res=extract_pan_details(image_path=\"pan.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cb86fe-b3f8-4aca-bdfd-2ee90e21d3c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0490eb2-37fb-435c-a9e3-e6439f65b85f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kyc]",
   "language": "python",
   "name": "conda-env-kyc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
