{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cda160c-0af0-4739-82fe-8f62dd660ace",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Debugging API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6632fc29-92ce-4411-8e77-48f32304624a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sanje\\anaconda3\\envs\\kyc\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sanje\\anaconda3\\envs\\kyc\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:216: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sanje\\anaconda3\\envs\\kyc\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sanje\\anaconda3\\envs\\kyc\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sanje\\anaconda3\\envs\\kyc\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sanje\\anaconda3\\envs\\kyc\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n",
      "WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "2025-02-10 18:32:15 - INFO - [models.py:19] - models are initialized\n",
      "INFO:app.backend.utlis.models:models are initialized\n"
     ]
    }
   ],
   "source": [
    "from fastapi import APIRouter, UploadFile, File\n",
    "import asyncio\n",
    "import io\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ExifTags\n",
    "from app.backend.utlis.ocr import pan_ocr, adhar_ocr\n",
    "from app.backend.utlis.face import extract_face, enhance_image, match_faces_with_facenet\n",
    "from app.backend.utlis.logging_config import get_logger  # Import the global logger\n",
    "from typing import Optional, Dict\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f0ed176-7fe3-4348-bad7-75bbaed81386",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82d71e40-c0a2-4905-bcbb-1cfa2c513952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(image_data: bytes) -> Optional[np.ndarray]:\n",
    "    logger.info(f\"Received image data, length: {len(image_data)} bytes\")\n",
    "    if not image_data:\n",
    "        logger.error(\"Image data is empty or unreadable\")\n",
    "        return None\n",
    "    try:\n",
    "        image = Image.open(io.BytesIO(image_data))\n",
    "        try:\n",
    "            exif = image._getexif()\n",
    "            if exif:\n",
    "                for tag, value in exif.items():\n",
    "                    if ExifTags.TAGS.get(tag) == \"Orientation\":\n",
    "                        if value == 3:\n",
    "                            image = image.rotate(180, expand=True)\n",
    "                        elif value == 6:\n",
    "                            image = image.rotate(270, expand=True)\n",
    "                        elif value == 8:\n",
    "                            image = image.rotate(90, expand=True)\n",
    "        except Exception as exif_error:\n",
    "            logger.warning(f\"EXIF data not found or could not be processed: {exif_error}\")\n",
    "        image = image.convert(\"RGB\")  # Ensure no alpha channel\n",
    "        cv2_img = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "        logger.info(f\"Image processed successfully with shape {cv2_img.shape} in BGR format\")\n",
    "        return cv2_img\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing image: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37f83fb0-b15f-49df-b4ec-33fd2342daee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents() -> Dict[str, dict]:\n",
    "    logger.info(\"Processing documents started\")\n",
    "    try:\n",
    "#loading images\n",
    "        with open(\"pan.jpg\", \"rb\") as image_file:\n",
    "            pan_image_data = image_file.read()\n",
    "        with open(\"live.jpg\", \"rb\") as image_file:\n",
    "            live_image_data = image_file.read()\n",
    "        with open(\"adhar_front.jpg\", \"rb\") as image_file:\n",
    "            adhar_image_data = image_file.read()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading images: {e}\")\n",
    "        return {\"error\": \"Failed to read images\"}\n",
    "    try:\n",
    "# read_image(): converting images from bytes to np.ndarray\n",
    "        pan_img = read_image(pan_image_data)\n",
    "        adhar_img = read_image(adhar_image_data)\n",
    "        live_img = read_image(live_image_data)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading image data: {e}\")\n",
    "        return {\"error\": \"Error processing image data\"}\n",
    "# extract_face: esrGan,mtcnn:\n",
    "    #extract live_face using esrGan and then mtcnn\n",
    "    extracted_face = extract_face(live_img)\n",
    "    if extracted_face is None:\n",
    "        logger.warning(\"No face detected in live image\")\n",
    "        return {\"error\": \"No face detected in live image\"}\n",
    "    logger.info(\"Face successfully extracted from live image\")\n",
    "    try:\n",
    "# performing OCR:\n",
    "        pan_text= pan_ocr(image=pan_img)\n",
    "        adhar_text=adhar_ocr(image=adhar_img)\n",
    "        logger.info(\"OCR completed successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"OCR processing error: {e}\")\n",
    "        return {\"error\": \"Failed to perform OCR\"}\n",
    "# extract_face: esrGan,mtcnn:\n",
    "    #extract pan_face & adhar_face using esrGan and then mtcnn\n",
    "    adhar_face=extract_face(adhar_img)\n",
    "    pan_face =extract_face(pan_img)\n",
    "    if adhar_face is not None:\n",
    "        logger.info(\"Face extracted from Aadhar image\")\n",
    "    else:\n",
    "        logger.warning(\"No face detected in Aadhar image\")\n",
    "    if pan_face is not None:\n",
    "        logger.info(\"Face extracted from PAN image\")\n",
    "    else:\n",
    "        logger.warning(\"No face detected in PAN image\")\n",
    "# facenet: match_faces_with_facenet(): calculate face simmilarity score\n",
    "    match_adhar, score_adhar = match_faces_with_facenet(extracted_face, adhar_face) if adhar_face is not None else (False, None)\n",
    "    match_pan, score_pan = match_faces_with_facenet(extracted_face, pan_face) if pan_face is not None else (False, None)\n",
    "    logger.info(f\"Face match results - Aadhar: {match_adhar}, Score: {score_adhar}\")\n",
    "    logger.info(f\"Face match results - PAN: {match_pan}, Score: {score_pan}\")\n",
    "    return {\n",
    "        \"pan_ocr\": pan_text,\n",
    "        \"adhar_ocr\": adhar_text,\n",
    "        \"face_match_with_adhar\": {\"match\": match_adhar, \"score\": score_adhar},\n",
    "        \"face_match_with_pan\": {\"match\": match_pan, \"score\": score_pan},\n",
    "        \"extracted_pan_face\" : pan_face,\n",
    "        \"extracted_adhar_face\" : adhar_face,\n",
    "        \"pan_img\" : pan_img,\n",
    "        \"adhar_img\" : adhar_img,\n",
    "        \"live_img\" : live_img,\n",
    "        \"extracted_live_face\" : extracted_face,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4ca6fb-fe58-4f3d-8d4f-a29be2761096",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=process_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a047ce16-5c7c-48c6-a77f-9786ae4cc530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "arr_rgb = cv2.cvtColor(result['extracted_live_face'], cv2.COLOR_BGR2RGB)\n",
    "image = Image.fromarray(arr_rgb)\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5443ea88-42c1-40bd-892d-11976a3e155c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# extract face from id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "180a58f3-b30b-44cb-b69c-1d0d0eacd8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from mtcnn import MTCNN\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def extract_and_save_face(image_path: str, *, save_path: str = \"cropped_faces/\", padding: float = 0.35) -> bool:\n",
    "    \"\"\"\n",
    "    Detects a face in the given image, applies padding to the cropped face, saves it, and returns whether a face was found.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the input image.\n",
    "        save_path (str, keyword-only): Directory where cropped face images will be saved.\n",
    "        padding (float, keyword-only): Padding factor (0.1 = 10% extra around the face).\n",
    "\n",
    "    Returns:\n",
    "        bool: True if a face was found and saved, False otherwise.\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    image: Optional[np.ndarray] = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to read image {image_path}\")\n",
    "        return False\n",
    "\n",
    "    # Initialize MTCNN detector\n",
    "    detector = MTCNN()\n",
    "\n",
    "    # Detect faces\n",
    "    faces = detector.detect_faces(image)\n",
    "\n",
    "    if not faces:\n",
    "        print(\"No face detected.\")\n",
    "        return False\n",
    "\n",
    "    # Ensure save directory exists\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    height, width, _ = image.shape  # Image dimensions\n",
    "\n",
    "    for i, face in enumerate(faces):  # Loop in case there are multiple faces\n",
    "        x, y, w, h = face[\"box\"]\n",
    "        print(f\"x :{x}, y : {y}, w : {w}, h : {h}\")\n",
    "\n",
    "        # Compute padding\n",
    "        pad_x = int(w * padding)\n",
    "        pad_y = int(h * padding)\n",
    "\n",
    "        # Expand bounding box\n",
    "        x1, y1 = max(x - pad_x, 0), max(y - pad_y, 0)\n",
    "        x2, y2 = min(x + w + pad_x, width), min(y + h + pad_y, height)\n",
    "\n",
    "        # Crop the padded face\n",
    "        cropped_face = image[y1:y2, x1:x2]\n",
    "\n",
    "        # Save cropped face\n",
    "        #face_filename = os.path.join(save_path, f\"face_{os.path.basename(image_path)}\")\n",
    "        face_filename = os.path.join(save_path, f\"face_{os.path.basename(image_path).replace('.jpg', '.png')}\")\n",
    "        cv2.imwrite(face_filename, cropped_face)\n",
    "        print(f\"✅ Face saved at: {face_filename} with padding {padding*100:.1f}%\")\n",
    "        return True  # Return True after saving the first detected face\n",
    "\n",
    "    return False  # If no face was saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4d270e53-1d2e-4698-a79a-ab0aaad70476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x :84, y : 102, w : 44, h : 55\n",
      "✅ Face saved at: cropped_faces/face_adhar_front.png with padding 35.0%\n",
      "Face extraction successful: True\n"
     ]
    }
   ],
   "source": [
    "success = extract_and_save_face(\"adhar_front.jpg\")\n",
    "print(f\"Face extraction successful: {success}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "355deaef-c10e-4653-bce2-015548456639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x :161, y : 425, w : 124, h : 147\n",
      "✅ Face saved at: cropped_faces/face_pan.png with padding 35.0%\n",
      "Face extraction successful: True\n"
     ]
    }
   ],
   "source": [
    "success = extract_and_save_face(\"pan.jpg\")\n",
    "print(f\"Face extraction successful: {success}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b92806d4-6592-4a54-b1fc-806a4a838113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x :462, y : 103, w : 381, h : 467\n",
      "✅ Face saved at: cropped_faces/face_live.png with padding 35.0%\n",
      "Face extraction successful: True\n"
     ]
    }
   ],
   "source": [
    "success = extract_and_save_face(\"live.jpg\")\n",
    "print(f\"Face extraction successful: {success}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b596a75a-1e02-4852-9c87-d6251935629b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# esrGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6e479ebd-2d73-45b5-9e2b-08835e8cbe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "\n",
    "\n",
    "# Load Pretrained ESRGAN Model from TensorFlow Hub\n",
    "srgan_model = hub.load(\"https://tfhub.dev/captain-pool/esrgan-tf2/1\")\n",
    "\n",
    "\n",
    "def enhance_image(image_path: str, *, save_path: str = \"enhanced_faces/\") -> bool:\n",
    "    \"\"\"\n",
    "    Enhances the quality of a face image using a Super-Resolution GAN (SRGAN).\n",
    "    Args:\n",
    "        image_path (str): Path to the cropped face image.\n",
    "        save_path (str, keyword-only): Directory where enhanced images will be saved.\n",
    "    Returns:\n",
    "        bool: True if enhancement is successful, False otherwise.\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to read image {image_path}\")\n",
    "        return False\n",
    "    # Convert to RGB and normalize (0-1)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0\n",
    "    #image/=255.0\n",
    "    # Resize to match ESRGAN input requirements\n",
    "    #image_resized = cv2.resize(image, (50, 50))  # Model expects 50x50 input\n",
    "    image_resized = np.expand_dims(image, axis=0).astype(np.float32)\n",
    "    # Run through the ESRGAN model\n",
    "    enhanced = srgan_model(image_resized)[0]  # Get first output\n",
    "    # ✅ Convert EagerTensor to NumPy before processing\n",
    "    enhanced = np.array(enhanced)  \n",
    "    enhanced = (enhanced * 255.0).clip(0, 255).astype(np.uint8)  # Process as usual\n",
    "    # Ensure save directory exists\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    # Save enhanced image\n",
    "    enhanced_filename = os.path.join(save_path, f\"enhanced_{os.path.basename(image_path)}\")\n",
    "    cv2.imwrite(enhanced_filename, cv2.cvtColor(enhanced, cv2.COLOR_RGB2BGR))\n",
    "    print(f\"✅ Enhanced face saved at: {enhanced_filename}\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "bca29124-689e-485e-ac70-41022d7035f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Enhanced face saved at: enhanced_faces/enhanced_face_live.png\n",
      "Super-resolution enhancement successful: True\n"
     ]
    }
   ],
   "source": [
    "success = enhance_image(\"cropped_faces/face_live.png\")\n",
    "print(f\"Super-resolution enhancement successful: {success}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "15aefa02-615c-4ba9-97ed-89b4fff497b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Enhanced face saved at: enhanced_faces/enhanced_face_adhar_front.png\n",
      "Super-resolution enhancement successful: True\n"
     ]
    }
   ],
   "source": [
    "success = enhance_image(\"cropped_faces/face_adhar_front.png\")\n",
    "print(f\"Super-resolution enhancement successful: {success}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "2fd5ecc4-07b3-4d3a-85bf-1864d2e9a49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Enhanced face saved at: enhanced_faces/enhanced_face_pan.png\n",
      "Super-resolution enhancement successful: True\n"
     ]
    }
   ],
   "source": [
    "success = enhance_image(\"cropped_faces/face_pan.png\")\n",
    "print(f\"Super-resolution enhancement successful: {success}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284a8476-60b6-442c-93a3-b8be6eb151fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Face Matching- FaceNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "41bd687b-c57c-4eb1-89eb-f309b8785e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_facenet import FaceNet\n",
    "import numpy as np\n",
    "import cv2\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def match_faces_with_facenet(image1_path: str, image2_path: str):\n",
    "    \"\"\"\n",
    "    Compares two pre-processed faces and returns whether they match and the similarity score.\n",
    "\n",
    "    Args:\n",
    "        image1_path (str): Path to the first image (already cropped and enhanced).\n",
    "        image2_path (str): Path to the second image (already cropped and enhanced).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - bool: True if the faces match, False otherwise.\n",
    "            - float: The cosine similarity score between the two faces.\n",
    "    \"\"\"\n",
    "    # Load the images using OpenCV (ensure they are valid)\n",
    "    image1 = cv2.imread(image1_path)\n",
    "    image2 = cv2.imread(image2_path)\n",
    "    \n",
    "    if image1 is None:\n",
    "        raise ValueError(f\"Failed to load image at {image1_path}\")\n",
    "    if image2 is None:\n",
    "        raise ValueError(f\"Failed to load image at {image2_path}\")\n",
    "\n",
    "    # Resize the images to the size that FaceNet expects (160x160)\n",
    "    image1 = cv2.resize(image1, (160, 160))\n",
    "    image2 = cv2.resize(image2, (160, 160))\n",
    "\n",
    "    # Convert images to RGB (FaceNet expects RGB images, OpenCV loads in BGR by default)\n",
    "    image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "    image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Add an extra batch dimension: from (160, 160, 3) to (1, 160, 160, 3)\n",
    "    image1 = np.expand_dims(image1, axis=0)\n",
    "    image2 = np.expand_dims(image2, axis=0)\n",
    "\n",
    "    # Initialize the FaceNet model\n",
    "    facenet = FaceNet()\n",
    "\n",
    "    # Extract face embeddings\n",
    "    embedding1 = facenet.embeddings(image1)[0]\n",
    "    embedding2 = facenet.embeddings(image2)[0]\n",
    "\n",
    "    # Compute the cosine distance between the two embeddings\n",
    "    distance = cosine(embedding1, embedding2)\n",
    "\n",
    "    # Define a threshold for matching (typically, 0.6 to 0.7 is a common threshold)\n",
    "    threshold = 0.6\n",
    "\n",
    "    # Determine if faces match\n",
    "    faces_match = distance < threshold\n",
    "\n",
    "    return faces_match, distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f9487666-6c5f-4c06-a598-dd47283e5c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000025BDC1A5580> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000025BDC1A5580> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "Do the faces match? True\n",
      "Similarity Score (distance): 0.4027039408683777\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "image1_path = \"enhanced_faces/enhanced_face_pan.png\"\n",
    "image2_path = \"enhanced_faces/enhanced_face_live.png\"\n",
    "\n",
    "faces_match, similarity_score = match_faces_with_facenet(image1_path, image2_path)\n",
    "\n",
    "print(f\"Do the faces match? {faces_match}\")\n",
    "print(f\"Similarity Score (distance): {similarity_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "1b71e503-6431-4690-9d63-556db5802867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "Do the faces match? True\n",
      "Similarity Score (distance): 0.5981652736663818\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "image1_path = \"enhanced_faces/enhanced_face_adhar_front.jpg\"\n",
    "image2_path = \"enhanced_faces/enhanced_face_live.jpg\"\n",
    "\n",
    "faces_match, similarity_score = match_faces_with_facenet(image1_path, image2_path)\n",
    "\n",
    "print(f\"Do the faces match? {faces_match}\")\n",
    "print(f\"Similarity Score (distance): {similarity_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2c2b3d-78b8-4220-ab60-210b6d3f2da3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# OCR using easyocr "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f932037-4c11-4ff0-91c8-2dc170a4aae5",
   "metadata": {},
   "source": [
    "### PAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43f7634a-83b2-4e61-9324-37e20738117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import easyocr\n",
    "import re\n",
    "import logging\n",
    "\n",
    "# Configure logging (PEP 282)\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "\n",
    "def preprocess_image(image_path: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Preprocess the image by converting it to grayscale and applying adaptive thresholding.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Preprocessed thresholded image.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "\n",
    "    if img is None:\n",
    "        logging.error(f\"Failed to load image: {image_path}\")\n",
    "        raise FileNotFoundError(f\"Image not found at {image_path}\")\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply adaptive thresholding to enhance text visibility\n",
    "    thresh = cv2.adaptiveThreshold(\n",
    "        gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 31, 2\n",
    "    )\n",
    "\n",
    "    logging.info(\"Image preprocessing completed.\")\n",
    "    return thresh\n",
    "\n",
    "\n",
    "def extract_pan_details(*, image_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extract important details from a PAN card image using OCR.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        dict: Extracted information including ID Number, Date of Birth, Name, and Father's Name.\n",
    "    \"\"\"\n",
    "    reader = easyocr.Reader([\"en\"])  # Load OCR reader for English\n",
    "\n",
    "    # Preprocess the image\n",
    "    processed_img = preprocess_image(image_path)\n",
    "\n",
    "    # Extract text using EasyOCR\n",
    "    results = reader.readtext(processed_img, detail=0)  # Extract text only\n",
    "\n",
    "    extracted_info = {\n",
    "        \"ID Number\": None,\n",
    "        \"Date of Birth\": None,\n",
    "        \"Name\": None,\n",
    "        \"Father's Name\": None,\n",
    "    }\n",
    "\n",
    "    name_found = False  # Track if Name is found\n",
    "    father_name_found = False  # Track if Father's Name is found\n",
    "\n",
    "    for i, text in enumerate(results):\n",
    "        text = text.strip()\n",
    "\n",
    "        # Extract PAN Number (Format: ABCDE1234F)\n",
    "        if re.match(r\"^[A-Z]{5}[0-9]{4}[A-Z]$\", text):\n",
    "            extracted_info[\"ID Number\"] = text\n",
    "\n",
    "        # Extract Date of Birth (Format: DD/MM/YYYY)\n",
    "        if re.match(r\"\\d{2}/\\d{2}/\\d{4}\", text):\n",
    "            extracted_info[\"Date of Birth\"] = text\n",
    "\n",
    "        # Extract Name (Assuming it appears after \"Name\" keyword)\n",
    "        if \"name\" in text.lower() and i + 1 < len(results) and not name_found:\n",
    "            extracted_info[\"Name\"] = results[i + 1].strip()\n",
    "            name_found = True\n",
    "\n",
    "        # Extract Father's Name (Appears after \"Father's Name\" or \"पिता का नाम\")\n",
    "        if \"father\" in text.lower() and i + 1 < len(results) and not father_name_found:\n",
    "            extracted_info[\"Father's Name\"] = results[i + 1].strip()\n",
    "            father_name_found = True\n",
    "\n",
    "    logging.info(f\"Extracted details: {extracted_info}\")\n",
    "    return extracted_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ef8ccd-3ec4-44c2-b145-e4fa5326b3e7",
   "metadata": {},
   "source": [
    "### adhar card front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bf2c2e4-d34d-4237-9232-57314f12fa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ext(image_path):\n",
    "    \"\"\"\n",
    "    Extract important details from an Aadhaar card image using OCR.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        dict: Extracted information including Aadhaar Number, Name, Date of Birth, and Gender.\n",
    "    \"\"\"\n",
    "    reader = easyocr.Reader([\"en\"])  # Load OCR reader for English & Hindi\n",
    "\n",
    "    # Preprocess the image\n",
    "    processed_img = preprocess_image(image_path)\n",
    "\n",
    "    # Extract text using EasyOCR\n",
    "    results = reader.readtext(processed_img, detail=0)  # Extract text only\n",
    "\n",
    "    extracted_info = {\n",
    "        \"Aadhaar Number\": None,\n",
    "        \"Date of Birth\": None,\n",
    "        \"Gender\": None,\n",
    "    }\n",
    "\n",
    "    name_candidates = []  # Track if Name is found\n",
    "    gender_found = False  # Track if Gender is found\n",
    "    \n",
    "    for i, text in enumerate(results):\n",
    "        text = text.strip()\n",
    "\n",
    "        # Extract Aadhaar Number (Format: 12-digit number)\n",
    "        if re.match(r\"^\\d{4}\\s?\\d{4}\\s?\\d{4}$\", text):\n",
    "            extracted_info[\"Aadhaar Number\"] = text.replace(\" \", \"\")  # Remove spaces\n",
    "\n",
    "        # Extract Date of Birth (Format: DD/MM/YYYY)\n",
    "        match=re.search(r\"\\b(\\d{2}/\\d{2}/\\d{4})\\b\", text)\n",
    "        if match:\n",
    "            extracted_info[\"Date of Birth\"] = match.group(1)\n",
    "\n",
    "\n",
    "\n",
    "        # Extract Gender (Look for \"Male\", \"Female\" or \"पुरुष\", \"महिला\")\n",
    "        if not gender_found:\n",
    "            if \"male\" in text.lower():\n",
    "                extracted_info[\"Gender\"] = \"Male\"\n",
    "                gender_found = True\n",
    "            elif \"female\" in text.lower():\n",
    "                extracted_info[\"Gender\"] = \"Female\"\n",
    "                gender_found = True\n",
    "\n",
    "    return extracted_info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2207be3f-7ffc-430b-874e-8704b133b539",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    }
   ],
   "source": [
    "res=extract_pan_details(image_path=\"pan.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cb86fe-b3f8-4aca-bdfd-2ee90e21d3c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49fd4cec-9ac1-4a7d-a8b4-872d4626441b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# DEBUGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eab04b1-f53a-43e6-bae4-1c6984a6244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load images\n",
    "# read_image()\n",
    "#extract_face() OR esrGAN and mtcnn\n",
    "#OCR\n",
    "#facenet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84858291-d1a6-44c5-b72e-83aec930c0a1",
   "metadata": {},
   "source": [
    "###### read jpg files into bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3904c1b-5d6a-4c06-8db1-e3b4ed76dbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pan.jpg\", \"rb\") as image_file:\n",
    "    pan_image_data = image_file.read()\n",
    "\n",
    "with open(\"live.jpg\", \"rb\") as image_file:\n",
    "    live_image_data = image_file.read()\n",
    "\n",
    "with open(\"adhar_front.jpg\", \"rb\") as image_file:\n",
    "    adhar_image_data = image_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2947192-d093-49cf-b8cb-2340d011318d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 21:35:27 - INFO - [486770899.py:2] - Received image data, length: 866002 bytes\n",
      "INFO:__main__:Received image data, length: 866002 bytes\n",
      "2025-02-09 21:35:27 - INFO - [486770899.py:23] - Image processed successfully with shape (1241, 1754, 3)\n",
      "INFO:__main__:Image processed successfully with shape (1241, 1754, 3)\n",
      "2025-02-09 21:35:27 - INFO - [486770899.py:2] - Received image data, length: 126576 bytes\n",
      "INFO:__main__:Received image data, length: 126576 bytes\n",
      "2025-02-09 21:35:27 - INFO - [486770899.py:23] - Image processed successfully with shape (720, 1280, 3)\n",
      "INFO:__main__:Image processed successfully with shape (720, 1280, 3)\n",
      "2025-02-09 21:35:27 - INFO - [486770899.py:2] - Received image data, length: 55807 bytes\n",
      "INFO:__main__:Received image data, length: 55807 bytes\n",
      "2025-02-09 21:35:27 - INFO - [486770899.py:23] - Image processed successfully with shape (344, 543, 3)\n",
      "INFO:__main__:Image processed successfully with shape (344, 543, 3)\n"
     ]
    }
   ],
   "source": [
    "pan=read_image(pan_image_data)\n",
    "live=read_image(live_image_data)\n",
    "adhar=read_image(adhar_image_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d033d6ff-e220-41da-8da6-78570a3b5b15",
   "metadata": {},
   "source": [
    "###### using notebook functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "57e46a83-61d3-4628-8a97-2d755ceeadce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from typing import Optional\n",
    "def extract_and_save_face(image: np.ndarray , padding: float = 0.35) -> Optional[np.ndarray]:\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to read image\")\n",
    "        return False\n",
    "    faces = detector.detect_faces(image)\n",
    "    if not faces:\n",
    "        print(\"No face detected.\")\n",
    "        return False\n",
    "    height, width, _ = image.shape \n",
    "    for i, face in enumerate(faces): \n",
    "        x, y, w, h = face[\"box\"] \n",
    "        print(f\"x :{x}, y : {y}, w : {w}, h : {h}\")\n",
    "        pad_x = int(w * padding)\n",
    "        pad_y = int(h * padding)\n",
    "        x1, y1 = max(x - pad_x, 0), max(y - pad_y, 0)\n",
    "        x2, y2 = min(x + w + pad_x, width), min(y + h + pad_y, height)\n",
    "        cropped_face = image[y1:y2, x1:x2]\n",
    "        return cropped_face\n",
    "    return None \n",
    "def enhance_image(image: np.ndarray) -> Optional[np.ndarray]:\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to read image\")\n",
    "        return False\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0\n",
    "    image_resized = np.expand_dims(image, axis=0).astype(np.float32)\n",
    "    enhanced = srgan_model(image_resized)[0]  \n",
    "    enhanced = np.array(enhanced)  \n",
    "    enhanced = (enhanced * 255.0).clip(0, 255).astype(np.uint8)  # Process as usual\n",
    "    enh=cv2.cvtColor(enhanced, cv2.COLOR_RGB2BGR)\n",
    "    return enh\n",
    "def match_faces_with_facenet(image1: np.ndarray, image2: np.ndarray):\n",
    "    if image1 is None:\n",
    "        raise ValueError(f\"Failed to load image\")\n",
    "    if image2 is None:\n",
    "        raise ValueError(f\"Failed to load image\")\n",
    "    image1 = cv2.resize(image1, (160, 160))\n",
    "    image2 = cv2.resize(image2, (160, 160))\n",
    "    image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "    image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "    image1 = np.expand_dims(image1, axis=0)\n",
    "    image2 = np.expand_dims(image2, axis=0)\n",
    "    embedding1 = facenet.embeddings(image1)[0]\n",
    "    embedding2 = facenet.embeddings(image2)[0]\n",
    "    distance = cosine(embedding1, embedding2)\n",
    "    threshold = 0.6\n",
    "    faces_match = distance < threshold\n",
    "    return faces_match, distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cfa09b3-f9a9-44a2-b269-52cc02af8b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.backend.utlis.models import mtcnn_detector as detector\n",
    "from app.backend.utlis.models import srgan_model\n",
    "from app.backend.utlis.models import facenet\n",
    "from app.backend.utlis.logging_config import get_logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8766cc-e71d-4047-bc39-94a84c52246e",
   "metadata": {},
   "source": [
    "###### using extract_face()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99125780-0ab9-4740-971f-0f8f4e64e9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x :161, y : 425, w : 124, h : 147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 21:36:04 - INFO - [face.py:96] - Image enhancement successful.\n",
      "INFO:app.backend.utlis.face:Image enhancement successful.\n",
      "2025-02-09 21:36:04 - INFO - [face.py:62] - Face extracted and enhanced with padding 35.0%\n",
      "INFO:app.backend.utlis.face:Face extracted and enhanced with padding 35.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x :462, y : 103, w : 381, h : 467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 21:36:30 - INFO - [face.py:96] - Image enhancement successful.\n",
      "INFO:app.backend.utlis.face:Image enhancement successful.\n",
      "2025-02-09 21:36:30 - INFO - [face.py:62] - Face extracted and enhanced with padding 35.0%\n",
      "INFO:app.backend.utlis.face:Face extracted and enhanced with padding 35.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x :84, y : 102, w : 44, h : 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 21:36:32 - INFO - [face.py:96] - Image enhancement successful.\n",
      "INFO:app.backend.utlis.face:Image enhancement successful.\n",
      "2025-02-09 21:36:32 - INFO - [face.py:62] - Face extracted and enhanced with padding 35.0%\n",
      "INFO:app.backend.utlis.face:Face extracted and enhanced with padding 35.0%\n"
     ]
    }
   ],
   "source": [
    "extract_pan=extract_face(pan)\n",
    "extract_live=extract_face(live)\n",
    "extract_adhar=extract_face(adhar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9312d41-d017-41b5-bc0e-874b51beabd9",
   "metadata": {},
   "source": [
    "###### notbook files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbe571e1-d6f4-4f6e-991e-134d0c093952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x :161, y : 425, w : 124, h : 147\n",
      "x :462, y : 103, w : 381, h : 467\n",
      "x :84, y : 102, w : 44, h : 55\n"
     ]
    }
   ],
   "source": [
    "ext_pan=extract_and_save_face(pan)\n",
    "ext_live=extract_and_save_face(live)\n",
    "ext_adhar=extract_and_save_face(adhar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a80a41d0-ef35-4ec0-8073-bb92e571eda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "enh_pan=enhance_image(ext_pan)\n",
    "enh_live=enhance_image(ext_live)\n",
    "enh_adhar=enhance_image(ext_adhar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382e7830-9d4a-4b99-bb9d-b1543c8a7c3a",
   "metadata": {},
   "source": [
    "##### checking if the reult from two different ways are same or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130daf7d-bad2-4fda-93cf-07f1609c3b91",
   "metadata": {},
   "source": [
    "###### Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "844a34d4-c5a3-410f-be33-c4bbb234b7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of PAN\n",
      "extracted face dimensions from notebook : (249, 210, 3) and type : <class 'numpy.ndarray'>\n",
      "enhanced face dimensions from notebook : (996, 840, 3) and type : <class 'numpy.ndarray'>\n",
      "extracted & enhanced face dimensions : (996, 840, 3) and type : <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of PAN\")\n",
    "print(f\"extracted face dimensions from notebook : {ext_pan.shape} and type : {type(ext_pan)}\")\n",
    "print(f\"enhanced face dimensions from notebook : {enh_pan.shape} and type : {type(enh_pan)}\")\n",
    "print(f\"extracted & enhanced face dimensions : {extract_pan.shape} and type : {type(extract_pan)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b0ccd17-b2f1-469a-9bd5-764aa4e8fa08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of adhar\n",
      "extracted face dimensions from notebook : (93, 74, 3)\n",
      "enhanced face dimensions from notebook : (372, 296, 3)\n",
      "extracted & enhanced face dimensions : (372, 296, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of adhar\")\n",
    "print(f\"extracted face dimensions from notebook : {ext_adhar.shape}\")\n",
    "print(f\"enhanced face dimensions from notebook : {enh_adhar.shape}\")\n",
    "print(f\"extracted & enhanced face dimensions : {extract_adhar.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f752b4c-545a-408b-b670-40be81d72a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of live\n",
      "extracted face dimensions from notebook : (720, 647, 3)\n",
      "enhanced face dimensions from notebook : (2880, 2588, 3)\n",
      "extracted & enhanced face dimensions : (2880, 2588, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of live\")\n",
    "print(f\"extracted face dimensions from notebook : {ext_live.shape}\")\n",
    "print(f\"enhanced face dimensions from notebook : {enh_live.shape}\")\n",
    "print(f\"extracted & enhanced face dimensions : {extract_live.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f142e30-587d-4160-9a76-26f2f19af2a8",
   "metadata": {},
   "source": [
    "###### if both are same or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0528bf3-1dc0-41cb-b489-27e2dc6b23e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check for PAN(floating point precision) : True\n",
      "check for PAN : True\n",
      "check for adhar(floating point precision) : True\n",
      "check for adhar : True\n",
      "check for LIVE(floating point precision) : True\n",
      "check for LIVE : True\n"
     ]
    }
   ],
   "source": [
    "print(f\"check for PAN(floating point precision) : {np.array_equal(extract_pan, enh_pan)}\")\n",
    "print(f\"check for PAN : {np.allclose(extract_pan, enh_pan, atol=1e-8)}\")\n",
    "print(f\"check for adhar(floating point precision) : {np.array_equal(extract_adhar, enh_adhar)}\")\n",
    "print(f\"check for adhar : {np.allclose(extract_adhar, enh_adhar, atol=1e-8)}\")\n",
    "print(f\"check for LIVE(floating point precision) : {np.array_equal(extract_live, enh_live)}\")\n",
    "print(f\"check for LIVE : {np.allclose(extract_live, enh_live, atol=1e-8)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7598b37-ac1c-41a6-a18f-d106d9e6df60",
   "metadata": {},
   "source": [
    "##### debugging face matching "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8192211-a5a4-45fb-91aa-23fb2b688b54",
   "metadata": {},
   "source": [
    "###### face matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f48784cd-df98-4e82-bef9-2a5fe285a8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n"
     ]
    }
   ],
   "source": [
    "panXlive=match_faces_with_facenet(enh_pan,enh_live)\n",
    "adharXlive=match_faces_with_facenet(enh_adhar,enh_live)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3b04d94d-a12f-4b51-957b-ffa3b5b55a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, 0.6518564314756645)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adharXlive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "af47b3ae-fb10-42ec-976d-5d84a710778f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 0.4027039408683777)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panXlive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7b36b7-a3a4-4ab7-9c4c-0a9abb5cb654",
   "metadata": {},
   "source": [
    "###### checking why reults are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "70bb9318-d00c-45fa-86ab-449675796e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "crp_pan = cv2.imread(\"cropped_faces/face_pan.png\")\n",
    "crp_live = cv2.imread(\"cropped_faces/face_live.png\")\n",
    "crp_adhar = cv2.imread(\"cropped_faces/face_adhar_front.png\")\n",
    "fin_pan = cv2.imread(\"enhanced_faces/enhanced_face_pan.png\")\n",
    "fin_adhar = cv2.imread(\"enhanced_faces/enhanced_face_adhar_front.png\")\n",
    "fin_live = cv2.imread(\"enhanced_faces/enhanced_face_live.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "782b15c2-baba-42b9-b252-05df05b8cce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of PAN\n",
      "extracted face dimensions from notebook : (249, 210, 3) and type : <class 'numpy.ndarray'>\n",
      "enhanced face dimensions from notebook : (996, 840, 3) and type : <class 'numpy.ndarray'>\n",
      "extracted & enhanced face dimensions : (996, 840, 3) and type : <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of PAN\")\n",
    "print(f\"extracted face dimensions from notebook : {crp_pan.shape} and type : {type(crp_pan)}\")\n",
    "print(f\"enhanced face dimensions from notebook : {fin_pan.shape} and type : {type(fin_pan)}\")\n",
    "print(f\"extracted & enhanced face dimensions : {enh_pan.shape} and type : {type(enh_pan)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f91ec6a0-a930-4ec5-b3a4-8bbde8eb7540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check for PAN(floating point precision) : True\n",
      "check for PAN : True\n",
      "check for adhar(floating point precision) : True\n",
      "check for adhar : True\n",
      "check for LIVE(floating point precision) : True\n",
      "check for LIVE : True\n"
     ]
    }
   ],
   "source": [
    "print(f\"check for PAN(floating point precision) : {np.array_equal(fin_pan, enh_pan)}\")\n",
    "print(f\"check for PAN : {np.allclose(fin_pan, enh_pan, atol=1e-8)}\")\n",
    "print(f\"check for adhar(floating point precision) : {np.array_equal(fin_adhar, enh_adhar)}\")\n",
    "print(f\"check for adhar : {np.allclose(fin_adhar, enh_adhar, atol=1e-8)}\")\n",
    "print(f\"check for LIVE(floating point precision) : {np.array_equal(fin_live, enh_live)}\")\n",
    "print(f\"check for LIVE : {np.allclose(fin_live, enh_live, atol=1e-8)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "0c970a9b-9eee-458b-a14d-a0d97cbdeed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check for PAN(floating point precision) : True\n",
      "check for PAN : True\n",
      "check for adhar(floating point precision) : True\n",
      "check for adhar : True\n",
      "check for LIVE(floating point precision) : True\n",
      "check for LIVE : True\n"
     ]
    }
   ],
   "source": [
    "print(f\"check for PAN(floating point precision) : {np.array_equal(crp_pan, ext_pan)}\")\n",
    "print(f\"check for PAN : {np.allclose(crp_pan, ext_pan, atol=1e-8)}\")\n",
    "print(f\"check for adhar(floating point precision) : {np.array_equal(crp_adhar, ext_adhar)}\")\n",
    "print(f\"check for adhar : {np.allclose(crp_adhar, ext_adhar, atol=1e-8)}\")\n",
    "print(f\"check for LIVE(floating point precision) : {np.array_equal(crp_live, ext_live)}\")\n",
    "print(f\"check for LIVE : {np.allclose(crp_live, ext_live, atol=1e-8)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "744f2add-c63c-4a90-9e66-da2ff6c8f077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_rgb(image: np.ndarray) -> bool:\n",
    "    gray_bgr = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray_rgb = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    return np.array_equal(gray_bgr, gray_rgb)  # If equal, original was RGB\n",
    "def check(image: np.ndarray):\n",
    "    if is_rgb(image):\n",
    "        print(\"The image is in RGB format\")\n",
    "    else:\n",
    "        print(\"The image is in BGR format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "39afe687-bec1-4cb8-97c2-0229493a142f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image is in BGR format\n",
      "The image is in BGR format\n"
     ]
    }
   ],
   "source": [
    "check(enh_adhar)\n",
    "check(fin_adhar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1a3596-7758-4e34-9d0a-f2a3b69884f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PAN      x :161,  y : 425, w : 124, h : 147\n",
    "#pan      x :161,  y : 425, w : 124, h : 147\n",
    "#live     x :462,  y : 103, w : 381, h : 467\n",
    "#adhar    x :84,   y : 102, w : 44,  h : 55\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "795fb0db-1b3c-404e-b5ea-88b1e2e56180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "hist_a = cv2.calcHist([crp_adhar], [0], None, [256], [0, 256])\n",
    "hist_b = cv2.calcHist([ext_adhar], [0], None, [256], [0, 256])\n",
    "\n",
    "print(np.allclose(hist_a, hist_b, atol=1e-5))  # Should return True if only minor pixel shifts exist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e16e94-e627-40de-925e-1ccf3f9888cf",
   "metadata": {},
   "source": [
    "# streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57d2367b-37e8-442d-be91-085fec74541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "def main():\n",
    "    st.title(\"KYC Webcam Capture\")\n",
    "\n",
    "    # Initialize session state for controlling webcam\n",
    "    if \"webcam_active\" not in st.session_state:\n",
    "        st.session_state.webcam_active = False\n",
    "    if \"captured_image\" not in st.session_state:\n",
    "        st.session_state.captured_image = None\n",
    "\n",
    "    # Button to open webcam\n",
    "    if not st.session_state.webcam_active:\n",
    "        if st.button(\"Take Picture\"):\n",
    "            st.session_state.webcam_active = True\n",
    "\n",
    "    # If webcam is active, open webcam\n",
    "    if st.session_state.webcam_active:\n",
    "        # Start webcam using OpenCV\n",
    "        cap = cv2.VideoCapture(0)  # 0 for default webcam\n",
    "\n",
    "        # Check if webcam is opened successfully\n",
    "        if not cap.isOpened():\n",
    "            st.error(\"Could not access webcam.\")\n",
    "            st.session_state.webcam_active = False\n",
    "            return\n",
    "\n",
    "        st.write(\"Press 'Capture Image' to take a snapshot.\")\n",
    "\n",
    "        # Placeholder for displaying the video feed\n",
    "        frame_placeholder = st.empty()\n",
    "        capture_button = st.button(\"Capture Image\")\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                st.error(\"Failed to grab frame.\")\n",
    "                break\n",
    "\n",
    "            # Convert OpenCV frame (BGR) to RGB\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Display the live video feed\n",
    "            frame_placeholder.image(frame_rgb, channels=\"RGB\")\n",
    "\n",
    "            # If \"Capture Image\" button is pressed, save the image\n",
    "            if capture_button:\n",
    "                st.session_state.captured_image = frame_rgb\n",
    "                st.session_state.webcam_active = False\n",
    "                break\n",
    "\n",
    "        cap.release()  # Release the webcam\n",
    "\n",
    "    # Display the captured image\n",
    "    if st.session_state.captured_image is not None:\n",
    "        st.subheader(\"Captured Image:\")\n",
    "        st.image(st.session_state.captured_image, channels=\"RGB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5087a821-0ee3-4ecf-97c5-8ac668d83a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import requests\n",
    "\n",
    "# API Endpoint (Update if different)\n",
    "API_URL = \"http://localhost:8000/results/proces s_documents/\"\n",
    "\n",
    "def send_images_to_api(live_image, pan_image, adhar_image):\n",
    "    \"\"\"Sends uploaded images to FastAPI endpoint.\"\"\"\n",
    "    \n",
    "    # Prepare files for API request\n",
    "    files = {\n",
    "        \"live_image\": (\"live.jpg\", live_image, \"image/jpeg\"),\n",
    "        \"pan_image\": (\"pan.jpg\", pan_image, \"image/jpeg\"),\n",
    "        \"adhar_image\": (\"adhar.jpg\", adhar_image, \"image/jpeg\"),\n",
    "    }\n",
    "\n",
    "    response = requests.post(API_URL, files=files)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return {\"error\": f\"API request failed with status code {response.status_code}\"}\n",
    "\n",
    "def main():\n",
    "    st.title(\"📄 KYC Verification System\")\n",
    "\n",
    "    # Upload Images\n",
    "    st.subheader(\"📤 Upload Required Documents\")\n",
    "\n",
    "    live_image = st.file_uploader(\"Upload Live Image\", type=[\"jpg\", \"png\", \"jpeg\"])\n",
    "    pan_image = st.file_uploader(\"Upload PAN Card Image\", type=[\"jpg\", \"png\", \"jpeg\"])\n",
    "    adhar_image = st.file_uploader(\"Upload Aadhaar Card Image\", type=[\"jpg\", \"png\", \"jpeg\"])\n",
    "\n",
    "    # Submit Button\n",
    "    if st.button(\"Submit for Verification\"):\n",
    "        if live_image and pan_image and adhar_image:\n",
    "            st.write(\"📡 Sending images to API...\")\n",
    "\n",
    "            # Read image bytes\n",
    "            response = send_images_to_api(live_image.read(), pan_image.read(), adhar_image.read())\n",
    "\n",
    "            # Display API response\n",
    "            st.subheader(\"✅ API Response:\")\n",
    "            st.json(response)\n",
    "        else:\n",
    "            st.error(\"❌ Please upload all three images before submitting.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f292c683-be80-4077-9a6f-7d0a9c96a79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import cv2\n",
    "import numpy as np\n",
    "from app.backend.utlis.logging_config import get_logger\n",
    "logger=get_logger(__name__)\n",
    "\n",
    "# Initialize session state variables\n",
    "if \"captured_image_bytes\" not in st.session_state:\n",
    "    st.session_state.captured_image_bytes = None\n",
    "if \"pan_upload\" not in st.session_state:\n",
    "    st.session_state.pan_upload = None\n",
    "if \"adhar_upload\" not in st.session_state:\n",
    "    st.session_state.adhar_upload = None\n",
    "if \"image_captured\" not in st.session_state:\n",
    "    st.session_state.image_captured = False\n",
    "\n",
    "# Function to capture an image from webcam\n",
    "def capture_image():\n",
    "    logger.info(\"Attempting to access webcam\")\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        logger.error(\"Webcam not found or could not be accessed\")\n",
    "        st.error(\"Webcam not accessible. Please allow camera access.\")\n",
    "        return\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "\n",
    "    if ret:\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        _, buffer = cv2.imencode(\".jpg\", frame_rgb)  # Convert to JPEG\n",
    "        st.session_state.captured_image_bytes = buffer.tobytes()  # Store in session state\n",
    "        st.session_state.image_captured = True  # Mark as captured\n",
    "        st.image(frame_rgb, caption=\"Captured Image\", use_column_width=True)\n",
    "        logger.info(\"Image captured and stored in session state\")\n",
    "    else:\n",
    "        logger.error(\"Failed to capture image from webcam\")\n",
    "\n",
    "# UI Layout\n",
    "st.title(\"Document Upload Portal\")\n",
    "\n",
    "# Capture Image Button\n",
    "if st.button(\"Capture Image\"):\n",
    "    capture_image()\n",
    "\n",
    "# File uploaders\n",
    "pan_upload = st.file_uploader(\"Upload PAN Card\", type=[\"jpg\", \"png\", \"jpeg\"])\n",
    "if pan_upload:\n",
    "    st.session_state.pan_upload = pan_upload\n",
    "    logger.info(\"PAN card uploaded successfully\")\n",
    "\n",
    "adhar_upload = st.file_uploader(\"Upload Aadhaar Card\", type=[\"jpg\", \"png\", \"jpeg\"])\n",
    "if adhar_upload:\n",
    "    st.session_state.adhar_upload = adhar_upload\n",
    "    logger.info(\"Aadhaar card uploaded successfully\")\n",
    "\n",
    "# Submit button\n",
    "if st.button(\"Submit\"):\n",
    "    logger.info(f\"Captured Image in session: {st.session_state.get('captured_image_bytes') is not None}\")\n",
    "    logger.info(f\"PAN Image uploaded: {st.session_state.pan_upload is not None}\")\n",
    "    logger.info(f\"Aadhaar Image uploaded: {st.session_state.adhar_upload is not None}\")\n",
    "\n",
    "    # Check if all images are available\n",
    "    if st.session_state.get(\"captured_image_bytes\") and st.session_state.pan_upload and st.session_state.adhar_upload:\n",
    "        st.success(\"✅ Submission successful! All required documents uploaded.\")\n",
    "        logger.info(\"✅ Submission successful!\")\n",
    "    else:\n",
    "        st.error(\"❌ Submission failed - One or more images missing\")\n",
    "        logger.error(\"❌ Submission failed - One or more images missing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d83b2e-2c70-4776-b346-abdabcb645d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import cv2\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from app.logging_config import get_logger\n",
    "\n",
    "# Initialize Logger\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "# FastAPI API URL\n",
    "API_URL = \"http://localhost:8000/results/process_documents/\"\n",
    "\n",
    "# Initialize session state variables\n",
    "if \"captured_image_bytes\" not in st.session_state:\n",
    "    st.session_state.captured_image_bytes = None\n",
    "if \"pan_upload\" not in st.session_state:\n",
    "    st.session_state.pan_upload = None\n",
    "if \"adhar_upload\" not in st.session_state:\n",
    "    st.session_state.adhar_upload = None\n",
    "if \"image_captured\" not in st.session_state:\n",
    "    st.session_state.image_captured = False\n",
    "if \"submitted\" not in st.session_state:\n",
    "    st.session_state.submitted = False  # Prevent multiple submissions\n",
    "\n",
    "# Function to capture an image from the webcam\n",
    "def capture_image():\n",
    "    logger.info(\"📸 Attempting to capture live image from webcam...\")\n",
    "    st.write(\"📸 Capturing image from webcam...\")\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        st.error(\"❌ Webcam not accessible. Please allow camera access.\")\n",
    "        logger.error(\"❌ Webcam not accessible. Check permissions or connection.\")\n",
    "        return\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "\n",
    "    if ret:\n",
    "        logger.info(\"✅ Successfully captured image from webcam.\")\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        _, buffer = cv2.imencode(\".jpg\", frame_rgb)  # Convert to JPEG\n",
    "        st.session_state.captured_image_bytes = buffer.tobytes()  # Store in session state\n",
    "        st.session_state.image_captured = True  # Mark as captured\n",
    "\n",
    "        st.image(frame_rgb, caption=\"📷 Captured Live Image\", use_column_width=True)\n",
    "        st.success(\"✅ Image captured successfully!\")\n",
    "    else:\n",
    "        st.error(\"❌ Failed to capture image from webcam.\")\n",
    "        logger.error(\"❌ Failed to capture image from webcam.\")\n",
    "\n",
    "# Function to send images to FastAPI backend (Synchronous)\n",
    "def send_images_to_api():\n",
    "    if st.session_state.submitted:\n",
    "        st.warning(\"⚠️ Submission already in progress. Please wait...\")\n",
    "        logger.warning(\"⚠️ Submission already in progress. Preventing duplicate requests.\")\n",
    "        return\n",
    "\n",
    "    logger.info(\"🚀 Preparing to send images to API...\")\n",
    "\n",
    "    if not st.session_state.captured_image_bytes or not st.session_state.pan_upload or not st.session_state.adhar_upload:\n",
    "        st.error(\"❌ Submission failed - Please upload all required images\")\n",
    "        logger.error(\"❌ Missing images. Submission aborted.\")\n",
    "        return\n",
    "\n",
    "    st.session_state.submitted = True  # Prevent multiple submissions\n",
    "\n",
    "    # Convert images to file-like objects\n",
    "    files = {\n",
    "        \"pan_image\": (\"pan.jpg\", st.session_state.pan_upload.getvalue(), \"image/jpeg\"),\n",
    "        \"adhar_image\": (\"adhar.jpg\", st.session_state.adhar_upload.getvalue(), \"image/jpeg\"),\n",
    "        \"live_image\": (\"live.jpg\", BytesIO(st.session_state.captured_image_bytes), \"image/jpeg\"),\n",
    "    }\n",
    "\n",
    "    logger.info(\"📤 Sending images to FastAPI endpoint...\")\n",
    "\n",
    "    # Send request to API\n",
    "    try:\n",
    "        response = requests.post(API_URL, files=files)\n",
    "        logger.info(f\"✅ API Response Received with status code {response.status_code}\")\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            st.success(\"✅ Documents processed successfully!\")\n",
    "\n",
    "            # Display results\n",
    "            st.subheader(\"📜 OCR Results\")\n",
    "            st.write(f\"**PAN OCR:** {result.get('pan_ocr', 'N/A')}\")\n",
    "            st.write(f\"**Aadhar OCR:** {result.get('adhar_ocr', 'N/A')}\")\n",
    "\n",
    "            logger.info(\"✅ OCR Results displayed.\")\n",
    "\n",
    "            st.subheader(\"🆔 Face Match Results\")\n",
    "            face_match_adhar = result.get(\"face_match_with_adhar\", {})\n",
    "            face_match_pan = result.get(\"face_match_with_pan\", {})\n",
    "\n",
    "            st.write(f\"**Face Match with Aadhaar:** {'✅ Matched' if face_match_adhar.get('match') else '❌ Not Matched'}\")\n",
    "            st.write(f\"**Match Score (Aadhaar):** {face_match_adhar.get('score', 'N/A')}\")\n",
    "\n",
    "            st.write(f\"**Face Match with PAN:** {'✅ Matched' if face_match_pan.get('match') else '❌ Not Matched'}\")\n",
    "            st.write(f\"**Match Score (PAN):** {face_match_pan.get('score', 'N/A')}\")\n",
    "\n",
    "            logger.info(\"✅ Face match results displayed.\")\n",
    "\n",
    "        else:\n",
    "            st.error(f\"❌ Failed to process documents: {response.text}\")\n",
    "            logger.error(f\"❌ API Error: {response.text}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        st.error(f\"❌ Error: {e}\")\n",
    "        logger.error(f\"❌ Exception while sending request to API: {e}\")\n",
    "\n",
    "    st.session_state.submitted = False  # Reset submission state\n",
    "    logger.info(\"🔄 Submission state reset.\")\n",
    "\n",
    "# Streamlit UI Layout\n",
    "st.title(\"📄 Document Upload Portal\")\n",
    "logger.info(\"🎬 Streamlit UI loaded.\")\n",
    "\n",
    "# Capture Image Button\n",
    "if st.button(\"📸 Capture Live Image\"):\n",
    "    logger.info(\"🛑 Capture Live Image button clicked.\")\n",
    "    capture_image()\n",
    "\n",
    "# File uploaders for PAN and Aadhaar\n",
    "pan_upload = st.file_uploader(\"🆔 Upload PAN Card\", type=[\"jpg\", \"png\", \"jpeg\"])\n",
    "if pan_upload:\n",
    "    st.session_state.pan_upload = pan_upload\n",
    "    logger.info(\"📂 PAN image uploaded.\")\n",
    "\n",
    "adhar_upload = st.file_uploader(\"🆔 Upload Aadhaar Card\", type=[\"jpg\", \"png\", \"jpeg\"])\n",
    "if adhar_upload:\n",
    "    st.session_state.adhar_upload = adhar_upload\n",
    "    logger.info(\"📂 Aadhaar image uploaded.\")\n",
    "\n",
    "# Submit button\n",
    "if st.button(\"🚀 Submit to API\"):\n",
    "    logger.info(\"🛑 Submit button clicked. Processing...\")\n",
    "    st.write(\"📤 Sending data to API... Please wait.\")\n",
    "    send_images_to_api()  # Synchronous API call\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kyc]",
   "language": "python",
   "name": "conda-env-kyc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
