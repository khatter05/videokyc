{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5443ea88-42c1-40bd-892d-11976a3e155c",
   "metadata": {},
   "source": [
    "# extract face from id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "180a58f3-b30b-44cb-b69c-1d0d0eacd8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from mtcnn import MTCNN\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def extract_and_save_face(image_path: str, *, save_path: str = \"cropped_faces/\", padding: float = 0.35) -> bool:\n",
    "    \"\"\"\n",
    "    Detects a face in the given image, applies padding to the cropped face, saves it, and returns whether a face was found.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the input image.\n",
    "        save_path (str, keyword-only): Directory where cropped face images will be saved.\n",
    "        padding (float, keyword-only): Padding factor (0.1 = 10% extra around the face).\n",
    "\n",
    "    Returns:\n",
    "        bool: True if a face was found and saved, False otherwise.\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    image: Optional[np.ndarray] = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to read image {image_path}\")\n",
    "        return False\n",
    "\n",
    "    # Initialize MTCNN detector\n",
    "    detector = MTCNN()\n",
    "\n",
    "    # Detect faces\n",
    "    faces = detector.detect_faces(image)\n",
    "\n",
    "    if not faces:\n",
    "        print(\"No face detected.\")\n",
    "        return False\n",
    "\n",
    "    # Ensure save directory exists\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    height, width, _ = image.shape  # Image dimensions\n",
    "\n",
    "    for i, face in enumerate(faces):  # Loop in case there are multiple faces\n",
    "        x, y, w, h = face[\"box\"]\n",
    "\n",
    "        # Compute padding\n",
    "        pad_x = int(w * padding)\n",
    "        pad_y = int(h * padding)\n",
    "\n",
    "        # Expand bounding box\n",
    "        x1, y1 = max(x - pad_x, 0), max(y - pad_y, 0)\n",
    "        x2, y2 = min(x + w + pad_x, width), min(y + h + pad_y, height)\n",
    "\n",
    "        # Crop the padded face\n",
    "        cropped_face = image[y1:y2, x1:x2]\n",
    "\n",
    "        # Save cropped face\n",
    "        face_filename = os.path.join(save_path, f\"face_{os.path.basename(image_path)}\")\n",
    "        cv2.imwrite(face_filename, cropped_face)\n",
    "        print(f\"✅ Face saved at: {face_filename} with padding {padding*100:.1f}%\")\n",
    "        return True  # Return True after saving the first detected face\n",
    "\n",
    "    return False  # If no face was saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d270e53-1d2e-4698-a79a-ab0aaad70476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Face saved at: cropped_faces/face_adhar_front.jpg with padding 35.0%\n",
      "Face extraction successful: True\n"
     ]
    }
   ],
   "source": [
    "success = extract_and_save_face(\"adhar_front.jpg\")\n",
    "print(f\"Face extraction successful: {success}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b596a75a-1e02-4852-9c87-d6251935629b",
   "metadata": {},
   "source": [
    "# esrGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e479ebd-2d73-45b5-9e2b-08835e8cbe69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sanje\\anaconda3\\envs\\kyc\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 17:00:11,208 - WARNING - From C:\\Users\\sanje\\anaconda3\\envs\\kyc\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "2025-02-04 17:00:12,854 - INFO - Using C:\\Users\\sanje\\AppData\\Local\\Temp\\tfhub_modules to cache modules.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sanje\\anaconda3\\envs\\kyc\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 17:00:13,014 - WARNING - From C:\\Users\\sanje\\anaconda3\\envs\\kyc\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n",
      "2025-02-04 17:00:13,026 - INFO - Downloading TF-Hub Module 'https://tfhub.dev/captain-pool/esrgan-tf2/1'.\n",
      "2025-02-04 17:00:19,144 - INFO - Downloaded https://tfhub.dev/captain-pool/esrgan-tf2/1, Total size: 20.60MB\n",
      "2025-02-04 17:00:19,148 - INFO - Downloaded TF-Hub Module 'https://tfhub.dev/captain-pool/esrgan-tf2/1'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sanje\\anaconda3\\envs\\kyc\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 17:00:19,253 - WARNING - From C:\\Users\\sanje\\anaconda3\\envs\\kyc\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n",
      "2025-02-04 17:00:25,471 - INFO - Fingerprint not found. Saved model loading will continue.\n",
      "2025-02-04 17:00:25,472 - INFO - path_and_singleprint metric could not be logged. Saved model loading will continue.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "\n",
    "\n",
    "# Load Pretrained ESRGAN Model from TensorFlow Hub\n",
    "srgan_model = hub.load(\"https://tfhub.dev/captain-pool/esrgan-tf2/1\")\n",
    "\n",
    "\n",
    "def enhance_image(image_path: str, *, save_path: str = \"enhanced_faces/\") -> bool:\n",
    "    \"\"\"\n",
    "    Enhances the quality of a face image using a Super-Resolution GAN (SRGAN).\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the cropped face image.\n",
    "        save_path (str, keyword-only): Directory where enhanced images will be saved.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if enhancement is successful, False otherwise.\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to read image {image_path}\")\n",
    "        return False\n",
    "\n",
    "    # Convert to RGB and normalize (0-1)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0\n",
    "    #image/=255.0\n",
    "    # Resize to match ESRGAN input requirements\n",
    "    #image_resized = cv2.resize(image, (50, 50))  # Model expects 50x50 input\n",
    "    image_resized = np.expand_dims(image, axis=0).astype(np.float32)\n",
    "\n",
    "    # Run through the ESRGAN model\n",
    "    enhanced = srgan_model(image_resized)[0]  # Get first output\n",
    "\n",
    "    # ✅ Convert EagerTensor to NumPy before processing\n",
    "    enhanced = np.array(enhanced)  \n",
    "    enhanced = (enhanced * 255.0).clip(0, 255).astype(np.uint8)  # Process as usual\n",
    "\n",
    "    # Ensure save directory exists\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    # Save enhanced image\n",
    "    enhanced_filename = os.path.join(save_path, f\"enhanced_{os.path.basename(image_path)}\")\n",
    "    cv2.imwrite(enhanced_filename, cv2.cvtColor(enhanced, cv2.COLOR_RGB2BGR))\n",
    "    print(f\"✅ Enhanced face saved at: {enhanced_filename}\")\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bca29124-689e-485e-ac70-41022d7035f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Enhanced face saved at: enhanced_faces/enhanced_face_adhar_front.jpg\n",
      "Super-resolution enhancement successful: True\n"
     ]
    }
   ],
   "source": [
    "success = enhance_image(\"cropped_faces/face_adhar_front.jpg\")\n",
    "print(f\"Super-resolution enhancement successful: {success}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284a8476-60b6-442c-93a3-b8be6eb151fd",
   "metadata": {},
   "source": [
    "# Face Matching- FaceNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41bd687b-c57c-4eb1-89eb-f309b8785e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_facenet import FaceNet\n",
    "import numpy as np\n",
    "import cv2\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def match_faces_with_facenet(image1_path: str, image2_path: str):\n",
    "    \"\"\"\n",
    "    Compares two pre-processed faces and returns whether they match and the similarity score.\n",
    "\n",
    "    Args:\n",
    "        image1_path (str): Path to the first image (already cropped and enhanced).\n",
    "        image2_path (str): Path to the second image (already cropped and enhanced).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - bool: True if the faces match, False otherwise.\n",
    "            - float: The cosine similarity score between the two faces.\n",
    "    \"\"\"\n",
    "    # Load the images using OpenCV (ensure they are valid)\n",
    "    image1 = cv2.imread(image1_path)\n",
    "    image2 = cv2.imread(image2_path)\n",
    "    \n",
    "    if image1 is None:\n",
    "        raise ValueError(f\"Failed to load image at {image1_path}\")\n",
    "    if image2 is None:\n",
    "        raise ValueError(f\"Failed to load image at {image2_path}\")\n",
    "\n",
    "    # Resize the images to the size that FaceNet expects (160x160)\n",
    "    image1 = cv2.resize(image1, (160, 160))\n",
    "    image2 = cv2.resize(image2, (160, 160))\n",
    "\n",
    "    # Convert images to RGB (FaceNet expects RGB images, OpenCV loads in BGR by default)\n",
    "    image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "    image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Add an extra batch dimension: from (160, 160, 3) to (1, 160, 160, 3)\n",
    "    image1 = np.expand_dims(image1, axis=0)\n",
    "    image2 = np.expand_dims(image2, axis=0)\n",
    "\n",
    "    # Initialize the FaceNet model\n",
    "    facenet = FaceNet()\n",
    "\n",
    "    # Extract face embeddings\n",
    "    embedding1 = facenet.embeddings(image1)[0]\n",
    "    embedding2 = facenet.embeddings(image2)[0]\n",
    "\n",
    "    # Compute the cosine distance between the two embeddings\n",
    "    distance = cosine(embedding1, embedding2)\n",
    "\n",
    "    # Define a threshold for matching (typically, 0.6 to 0.7 is a common threshold)\n",
    "    threshold = 0.6\n",
    "\n",
    "    # Determine if faces match\n",
    "    faces_match = distance < threshold\n",
    "\n",
    "    return faces_match, distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9487666-6c5f-4c06-a598-dd47283e5c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 17:01:05,660 - INFO - Loading weights.\n",
      "2025-02-04 17:01:05,662 - INFO - Looking for C:\\Users\\sanje/.keras-facenet\\20180402-114759\\20180402-114759-weights.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sanje\\anaconda3\\envs\\kyc\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:216: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 17:01:10,147 - WARNING - From C:\\Users\\sanje\\anaconda3\\envs\\kyc\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:216: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
      "Do the faces match? True\n",
      "Similarity Score (distance): 0.5981652736663818\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "image1_path = \"enhanced_faces/enhanced_face_adhar_front.jpg\"\n",
    "image2_path = \"enhanced_faces/enhanced_face_live.jpg\"\n",
    "\n",
    "faces_match, similarity_score = match_faces_with_facenet(image1_path, image2_path)\n",
    "\n",
    "print(f\"Do the faces match? {faces_match}\")\n",
    "print(f\"Similarity Score (distance): {similarity_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2c2b3d-78b8-4220-ab60-210b6d3f2da3",
   "metadata": {},
   "source": [
    "# OCR using easyocr "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f932037-4c11-4ff0-91c8-2dc170a4aae5",
   "metadata": {},
   "source": [
    "### PAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43f7634a-83b2-4e61-9324-37e20738117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import easyocr\n",
    "import re\n",
    "import logging\n",
    "\n",
    "# Configure logging (PEP 282)\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "\n",
    "def preprocess_image(image_path: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Preprocess the image by converting it to grayscale and applying adaptive thresholding.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Preprocessed thresholded image.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "\n",
    "    if img is None:\n",
    "        logging.error(f\"Failed to load image: {image_path}\")\n",
    "        raise FileNotFoundError(f\"Image not found at {image_path}\")\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply adaptive thresholding to enhance text visibility\n",
    "    thresh = cv2.adaptiveThreshold(\n",
    "        gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 31, 2\n",
    "    )\n",
    "\n",
    "    logging.info(\"Image preprocessing completed.\")\n",
    "    return thresh\n",
    "\n",
    "\n",
    "def extract_pan_details(*, image_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extract important details from a PAN card image using OCR.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        dict: Extracted information including ID Number, Date of Birth, Name, and Father's Name.\n",
    "    \"\"\"\n",
    "    reader = easyocr.Reader([\"en\"])  # Load OCR reader for English\n",
    "\n",
    "    # Preprocess the image\n",
    "    processed_img = preprocess_image(image_path)\n",
    "\n",
    "    # Extract text using EasyOCR\n",
    "    results = reader.readtext(processed_img, detail=0)  # Extract text only\n",
    "\n",
    "    extracted_info = {\n",
    "        \"ID Number\": None,\n",
    "        \"Date of Birth\": None,\n",
    "        \"Name\": None,\n",
    "        \"Father's Name\": None,\n",
    "    }\n",
    "\n",
    "    name_found = False  # Track if Name is found\n",
    "    father_name_found = False  # Track if Father's Name is found\n",
    "\n",
    "    for i, text in enumerate(results):\n",
    "        text = text.strip()\n",
    "\n",
    "        # Extract PAN Number (Format: ABCDE1234F)\n",
    "        if re.match(r\"^[A-Z]{5}[0-9]{4}[A-Z]$\", text):\n",
    "            extracted_info[\"ID Number\"] = text\n",
    "\n",
    "        # Extract Date of Birth (Format: DD/MM/YYYY)\n",
    "        if re.match(r\"\\d{2}/\\d{2}/\\d{4}\", text):\n",
    "            extracted_info[\"Date of Birth\"] = text\n",
    "\n",
    "        # Extract Name (Assuming it appears after \"Name\" keyword)\n",
    "        if \"name\" in text.lower() and i + 1 < len(results) and not name_found:\n",
    "            extracted_info[\"Name\"] = results[i + 1].strip()\n",
    "            name_found = True\n",
    "\n",
    "        # Extract Father's Name (Appears after \"Father's Name\" or \"पिता का नाम\")\n",
    "        if \"father\" in text.lower() and i + 1 < len(results) and not father_name_found:\n",
    "            extracted_info[\"Father's Name\"] = results[i + 1].strip()\n",
    "            father_name_found = True\n",
    "\n",
    "    logging.info(f\"Extracted details: {extracted_info}\")\n",
    "    return extracted_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ef8ccd-3ec4-44c2-b145-e4fa5326b3e7",
   "metadata": {},
   "source": [
    "### adhar card front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bf2c2e4-d34d-4237-9232-57314f12fa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ext(image_path):\n",
    "    \"\"\"\n",
    "    Extract important details from an Aadhaar card image using OCR.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        dict: Extracted information including Aadhaar Number, Name, Date of Birth, and Gender.\n",
    "    \"\"\"\n",
    "    reader = easyocr.Reader([\"en\"])  # Load OCR reader for English & Hindi\n",
    "\n",
    "    # Preprocess the image\n",
    "    processed_img = preprocess_image(image_path)\n",
    "\n",
    "    # Extract text using EasyOCR\n",
    "    results = reader.readtext(processed_img, detail=0)  # Extract text only\n",
    "\n",
    "    extracted_info = {\n",
    "        \"Aadhaar Number\": None,\n",
    "        \"Date of Birth\": None,\n",
    "        \"Gender\": None,\n",
    "    }\n",
    "\n",
    "    name_candidates = []  # Track if Name is found\n",
    "    gender_found = False  # Track if Gender is found\n",
    "    \n",
    "    for i, text in enumerate(results):\n",
    "        text = text.strip()\n",
    "\n",
    "        # Extract Aadhaar Number (Format: 12-digit number)\n",
    "        if re.match(r\"^\\d{4}\\s?\\d{4}\\s?\\d{4}$\", text):\n",
    "            extracted_info[\"Aadhaar Number\"] = text.replace(\" \", \"\")  # Remove spaces\n",
    "\n",
    "        # Extract Date of Birth (Format: DD/MM/YYYY)\n",
    "        match=re.search(r\"\\b(\\d{2}/\\d{2}/\\d{4})\\b\", text)\n",
    "        if match:\n",
    "            extracted_info[\"Date of Birth\"] = match.group(1)\n",
    "\n",
    "\n",
    "\n",
    "        # Extract Gender (Look for \"Male\", \"Female\" or \"पुरुष\", \"महिला\")\n",
    "        if not gender_found:\n",
    "            if \"male\" in text.lower():\n",
    "                extracted_info[\"Gender\"] = \"Male\"\n",
    "                gender_found = True\n",
    "            elif \"female\" in text.lower():\n",
    "                extracted_info[\"Gender\"] = \"Female\"\n",
    "                gender_found = True\n",
    "\n",
    "    return extracted_info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2207be3f-7ffc-430b-874e-8704b133b539",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kyc]",
   "language": "python",
   "name": "conda-env-kyc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
